{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "import ppaquette_gym_doom\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.ndimage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this environment, the Doom player is standing on top of acid water, and needs to learn how to navigate and collect health packs to stay alive.  \n",
    "\n",
    "<img src=\"images/doom1.gif\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('ppaquette/DoomHealthGathering-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One method of reinforcement learning we can use to solve this problem is the REINFORCE with baselines algorithm. Reinforce is very simple -- the only data it needs includes states and rewards from an environment episode. Reinforce is called a policy gradient method because it solely evaluates and updates an agent's policy.\n",
    "\n",
    "Reinforce is considered a Monte Carlo method of learning, this means that the agent will collect data from an entire episode then perform calculations at the end of that episode.  In our case we will gather a batch of multiple episodes to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Parameters\n",
    "n_actions = 3\n",
    "n_epochs = 1500\n",
    "n = 0\n",
    "average = []\n",
    "step = 1\n",
    "batch_size = 5000\n",
    "render = False\n",
    "\n",
    "# Define our three actions of moving forward, turning left & turning right\n",
    "choice = [[0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "          [0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "          [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next define some of our hyper parameters that our neural network will use.\n",
    "\n",
    "Alpha is our usual learning rate and gamma is our rate of reward discounting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "alpha = 1e-4\n",
    "gamma = 0.99\n",
    "normalize_r = True\n",
    "save_path='models/healthGather.ckpt'\n",
    "value_scale = 0.5\n",
    "entropy_scale = 0.00\n",
    "gradient_clip = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reward discounting is a way of evaluating potential future rewards given the reward history from an agent. As the discount rate approaches zero, the agent is only concerned with immediate rewards and does not consider potential future rewards. We can write a simple function to evaluate a set of rewards from an episode, with the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply discount to episode rewards & normalize\n",
    "def discount(r, gamma, normal):\n",
    "    discount = np.zeros_like(r)\n",
    "    G = 0.0\n",
    "    for i in reversed(range(0, len(r))):\n",
    "        G = G * gamma + r[i]\n",
    "        discount[i] = G\n",
    "    # Normalize \n",
    "    if normal:\n",
    "        mean = np.mean(discount)\n",
    "        std = np.std(discount)\n",
    "        discount = (discount - mean) / (std)\n",
    "    return discount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s evaluate the following sets of rewards:\n",
    "    \n",
    "<img src=\"images/discounting.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will build our convolutional neural network for taking in a state and outputting action probabilities and state values.  We will have three actions to choose from: move forward, move right, and move left. The policy approximation is set up exactly the same as an image classifier, but instead of the outputs representing the confidence of a class, our outputs will represent our confidence in taking a certain action. Compared to large image classification models, when it comes to reinforcement learning, simple networks work best. \n",
    "\n",
    "We will use a very popular convnet also used for the famous DQN algorithm. Our network will input a processed resized image of 84x84 pixels, output 16 convolutions of a 8x8 kernel with a stride of 4, followed by 32 convolutions with a 4x4 kernel and a stride of 2, finished with a fully connected layer of 256 neurons. For the convolutional layers we will use ‘VALID’ padding which will shrink the image quite aggressively. \n",
    "\n",
    "Both our policy approximation and our value approximation will share the same convolutional neural network to calculate their values.  For input we will feed in the resized pixel values from the environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv Layers\n",
    "convs = [16,32]\n",
    "kerns = [8,8]\n",
    "strides = [4,4]\n",
    "pads = 'VALID'\n",
    "fc = 256\n",
    "activation = tf.nn.elu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for resizing image\n",
    "def resize(image):\n",
    "    # Greyscale Image\n",
    "    x = np.mean(image,-1)\n",
    "    # Normalize Pixel Values\n",
    "    x = x/255\n",
    "    x = scipy.misc.imresize(x, [84,84])\n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow Variables\n",
    "X = tf.placeholder(tf.float32, (None,84,84,1), name='X')\n",
    "Y = tf.placeholder(tf.int32, (None,), name='actions')\n",
    "R = tf.placeholder(tf.float32, (None,), name='reward')\n",
    "N = tf.placeholder(tf.float32, (None), name='episodes')\n",
    "D_R = tf.placeholder(tf.float32, (None,), name='discounted_reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Network\n",
    "# CONVOLUTION 1 - 1\n",
    "with tf.name_scope('conv1'):\n",
    "    filter1 = tf.truncated_normal([kerns[0], kerns[0], 1, convs[0]],\n",
    "                                  dtype=tf.float32,\n",
    "                                  stddev=1/np.sqrt(84**2))\n",
    "    filter1 = tf.Variable(filter1, name='weights1')\n",
    "    stride = [1,strides[0],strides[0],1]\n",
    "    conv1 = tf.nn.conv2d(X, filter1, stride, padding=pads)\n",
    "    biases1 = tf.Variable(tf.constant(0.0, shape=[convs[0]], dtype=tf.float32), name='biases1')\n",
    "    out = tf.nn.bias_add(conv1, biases1)\n",
    "    out1 = activation(out)\n",
    "    \n",
    "    \n",
    "# CONVOLUTION 1 - 2\n",
    "with tf.name_scope('conv2'):\n",
    "    shape = int(np.prod(out1.get_shape()[1:]))\n",
    "    filter2 = tf.truncated_normal([kerns[1], kerns[1], convs[0], convs[1]],\n",
    "                                  dtype=tf.float32,\n",
    "                                  stddev=1/np.sqrt(shape))\n",
    "    filter2 = tf.Variable(filter2, name='weights2')\n",
    "    stride = [1,strides[1],strides[1],1]\n",
    "    conv2 = tf.nn.conv2d(out1, filter2, stride, padding=pads)\n",
    "    biases2 = tf.Variable(tf.constant(0.0, shape=[convs[1]], dtype=tf.float32), name='biases2')\n",
    "    out = tf.nn.bias_add(conv2, biases2)\n",
    "    out2 = activation(out)\n",
    "    \n",
    "\n",
    "#FULLY CONNECTED\n",
    "with tf.name_scope('fc1') as scope:\n",
    "    shape = int(np.prod(out2.get_shape()[1:]))\n",
    "    fc1w = tf.truncated_normal([shape, fc], \n",
    "                               dtype=tf.float32, \n",
    "                               stddev=1/np.sqrt(shape))\n",
    "    fc1w = tf.Variable(fc1w, name='weights3')\n",
    "    fc1b = tf.Variable(tf.constant(1.0, shape=[fc], dtype=tf.float32), name='biases3')\n",
    "    flat = tf.reshape(out2, [-1, shape])\n",
    "    out = tf.nn.bias_add(tf.matmul(flat, fc1w), fc1b)\n",
    "    fc_1 = activation(out)\n",
    "    \n",
    "\n",
    "#POLICY FUNCTION\n",
    "with tf.name_scope('policy') as scope:\n",
    "    logitsW = tf.truncated_normal([fc, n_actions], \n",
    "                                  dtype=tf.float32, \n",
    "                                  stddev=1/np.sqrt(fc))\n",
    "    logitsW = tf.Variable(logitsW, name='weights4')\n",
    "    logitsB = tf.Variable(tf.constant(1.0, shape=[n_actions], dtype=tf.float32),\n",
    "                       trainable=True, name='biases4')\n",
    "    action_logits = tf.nn.bias_add(tf.matmul(fc_1, logitsW), logitsB)\n",
    "    calc_action = tf.multinomial(action_logits, 1)\n",
    "    aprob = tf.nn.softmax(action_logits)\n",
    "    \n",
    "#VALUE FUNCTION\n",
    "with tf.name_scope('value') as scope:\n",
    "    valueW = tf.truncated_normal([fc, 1], \n",
    "                                 dtype=tf.float32,\n",
    "                                 stddev=1)\n",
    "    valueW = tf.Variable(valueW, name='weights5')\n",
    "    value = tf.matmul(fc_1, valueW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will gather a batch of training data from multiple episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(batch_size, render):\n",
    "    \n",
    "    states, actions, rewards, rewardsFeed, discountedRewards = [], [], [], [], []\n",
    "    state = resize(env.reset())\n",
    "    episode_num = 0 \n",
    "    action_repeat = 3\n",
    "    reward = 0\n",
    "    \n",
    "    while True: \n",
    "        \n",
    "        if render:\n",
    "            env.render()\n",
    "        \n",
    "        # Run State Through Policy & Calculate Action\n",
    "        feed = {X: state.reshape(1, 84, 84, 1)}\n",
    "        action = sess.run(calc_action, feed_dict=feed)\n",
    "        action = action[0][0]\n",
    "        \n",
    "        # Perform Action\n",
    "        for i in range(action_repeat):\n",
    "            state2, reward2, done, info = env.step(choice[action])\n",
    "            reward += reward2\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Store Results\n",
    "        states.append(state)\n",
    "        rewards.append(reward)\n",
    "        actions.append(action)\n",
    "        \n",
    "        # Update Current State\n",
    "        reward = 0\n",
    "        state = resize(state2)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if done:\n",
    "            \n",
    "            episode_num += 1\n",
    "            \n",
    "            # Track Discounted Rewards\n",
    "            rewardsFeed.append(rewards)\n",
    "            discountedRewards.append(discount(rewards, gamma, normalize_r))\n",
    "            \n",
    "            if len(np.concatenate(rewardsFeed)) > batch_size:\n",
    "                break\n",
    "                \n",
    "            # Reset Environment\n",
    "            rewards = []\n",
    "            state = resize(env.reset())\n",
    "                         \n",
    "    return np.stack(states), np.stack(actions), np.concatenate(rewardsFeed), np.concatenate(discountedRewards), episode_num"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "So now that we have the model built, how are we going to have it learn? The solution is elegantly simple. We want to change the network's weights so that it will increase its confidence in what action to take, and the amount of change is based upon our baseline of how accurate our value estimation was. Overall we need to minimize our total loss.\n",
    "\n",
    "Implementing this in TensorFlow, we measure our loss by using the sparse_softmax_cross_entropy function.  The sparse means that our action labels are single integers and the the logits are our final policy output without an activation function.  This function calculates the softmax and log loss for us.  As confidence in an taken action approaches one, the loss approaches zero. \n",
    "\n",
    "We then multiply the cross entropy by the difference of our discounted reward and our value approximation to get our total policy gradient loss.  We calculate our value loss by using the common squared mean error loss.  We then add our losses to together to calculate our total loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward = tf.divide(tf.reduce_sum(R), N)\n",
    "\n",
    "# Define Losses\n",
    "pg_loss = tf.reduce_mean((D_R - value) * tf.nn.sparse_softmax_cross_entropy_with_logits(logits=action_logits, labels=Y))\n",
    "value_loss = value_scale * tf.reduce_mean(tf.square(D_R - value))\n",
    "entropy_loss = -entropy_scale * tf.reduce_sum(aprob * tf.exp(aprob))\n",
    "loss = pg_loss + value_loss - entropy_loss\n",
    "\n",
    "# Create Optimizer\n",
    "optimizer = tf.train.AdamOptimizer(alpha)\n",
    "grads = tf.gradients(loss, tf.trainable_variables())\n",
    "grads, _ = tf.clip_by_global_norm(grads, gradient_clip) # gradient clipping\n",
    "grads_and_vars = list(zip(grads, tf.trainable_variables()))\n",
    "train_op = optimizer.apply_gradients(grads_and_vars)\n",
    "\n",
    "# Initialize Session\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup TensorBoard Writer\n",
    "writer = tf.summary.FileWriter(\"/tmp/dpg\")\n",
    "tf.summary.scalar('Total_Loss', loss)\n",
    "tf.summary.scalar('PG_Loss', pg_loss)\n",
    "tf.summary.scalar('Entropy_Loss', entropy_loss)\n",
    "tf.summary.scalar('Value_Loss', value_loss)\n",
    "tf.summary.scalar('Reward_Mean', mean_reward)\n",
    "tf.summary.histogram('Conv1', filter1)\n",
    "tf.summary.histogram('Conv2', filter2)\n",
    "tf.summary.histogram('FC', fc1w)\n",
    "tf.summary.histogram('Logits', logitsW)\n",
    "tf.summary.histogram('Value', valueW)\n",
    "write_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model if exists\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "load_was_success = True \n",
    "try:\n",
    "    save_dir = '/'.join(save_path.split('/')[:-1])\n",
    "    ckpt = tf.train.get_checkpoint_state(save_dir)\n",
    "    load_path = ckpt.model_checkpoint_path\n",
    "    saver.restore(sess, load_path)\n",
    "except:\n",
    "    print(\"No saved model to load. Starting new session\")\n",
    "    writer.add_graph(sess.graph)\n",
    "    load_was_success = False\n",
    "else:\n",
    "    print(\"Loaded Model: {}\".format(load_path))\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    step = int(load_path.split('-')[-1])+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to train the agent. We feed our current state into the network and get our action by calling the tf.multinomial function.  We perform that action and store the state, action and future reward. We then store the new resized state2 as our current state and repeat this procedure until the end of the episode. We then append our state, action, and reward data into a new list, which we will use for feeding into the network, for evaluating an episode\n",
    "\n",
    "Depending on our intial weight initialization, our agent should eventually solve the environment in roughly 1000 training batches. OpenAI’s standard for solving the environment is getting an average reward of 1,000 over 100 consecutive trials. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "while step < n_epochs+1:\n",
    "    # Gather Training Data\n",
    "    print('Epoch', step)\n",
    "    s, a, r, d_r, n = rollout(batch_size,render)\n",
    "    mean_reward = np.sum(r)/n\n",
    "    average.append(mean_reward)\n",
    "    print('Training Episodes: {}  Average Reward: {:4.2f}  Total Average: {:4.2f}'.format(n, mean_reward, np.mean(average)))\n",
    "          \n",
    "    # Update Network\n",
    "    sess.run(train_op, feed_dict={X:s.reshape(len(s),84,84,1), Y:a, D_R: d_r})\n",
    "          \n",
    "    # Write TF Summaries\n",
    "    summary = sess.run(write_op, feed_dict={X:s.reshape(len(s),84,84,1), Y:a, D_R: d_r, R: r, N:n})\n",
    "    writer.add_summary(summary, step)\n",
    "    writer.flush()\n",
    "          \n",
    "    # Save Model\n",
    "    if step % 10 == 0:\n",
    "          print(\"SAVED MODEL\")\n",
    "          saver.save(sess, save_path, global_step=step)\n",
    "          \n",
    "    step += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is my agent after 1000 batches:\n",
    "\n",
    "<img src=\"images/doomFinal.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to test your agent’s confidence at any given frame, all you need to do is feed that state into the network and observe the output. Here, while facing just the wall, the agent had 73.5% confidence that the best action was to turn right and in the following picture the agent was 65% confident that going forward was the best action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = resize(env.reset())\n",
    "prob, val = sess.run([aprob, value], feed_dict={X: state.reshape(1, 84, 84, 1)})\n",
    "\n",
    "print('Turn Right: {:4.2f}  Turn Left: {:4.2f}  Move forward {:4.2f}'.format(prob[0][0],prob[0][2], prob[0][1]))\n",
    "print('Approximated State Value: {:4.4f}'.format(val[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are keen, you may think to yourself that this was way too easy and that 65% confidence for what seems like a clearly good move is not that great, and you would be right! This agent still has a very large amount of stochasticity (randomness) and that may be required for this simple agent to generalize well to the entire environment.\n",
    "\n",
    "In the case of my agent it seemed to develop a policy of: if it’s facing a wall to turn around and move around in a random spiral until it hits another wall. The agent also almost completely disregards turning left! I personally would not call this agent intelligent; the agent has a simple policy, but -- it has learned it on it’s own and it works! Unfortunately I was not able to achieve higher than a 1200 point average, the highest possible average you can achieve is 2100.\n",
    "\n",
    "<img src=\"images/tensorboard.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
