{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import ppaquette_gym_doom\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.ndimage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will use Python, TensorFlow and the reinforcement learning library Gym to solve the 3d Doom health gathering environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('ppaquette/DoomHealthGathering-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this environment the Doom player is standing on top of acid water and needs to learn how to navigate and collect health packs to stay alive. \n",
    "\n",
    "<img src=\"images/doom1.gif\">\n",
    "\n",
    "One method of reinforcement learning we can use to solve this problem is the REINFORCE algorithm.  Reinforce is very simple, the only data it needs is states and rewards from an environment episode. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforce is considered a Monte Carlo method of learning, this means that the agent will collect data from an entire episode then perform calculations at the end of that episode.  We will set up our environment training data as empty lists which we will append our data into for each step.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Parameters\n",
    "n_actions = 3\n",
    "n_episodes = 10000\n",
    "render = False\n",
    "train = True\n",
    "actionRepeat = 3\n",
    "\n",
    "# Environment Variables\n",
    "states, actions, rewards = [], [], []\n",
    "episode_num = 1\n",
    "G = 0\n",
    "reward = 0\n",
    "average, avg_loss = [], []\n",
    "counter = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alpha is our usual learning rate and gamma is our rate of reward discounting.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "alpha = 1e-5\n",
    "gamma = .99\n",
    "save_path='models/healthGather.ckpt'\n",
    "\n",
    "# Conv Layers\n",
    "convs = [16, 32]\n",
    "kerns = [8, 4]\n",
    "strides = [4, 2]\n",
    "pads = 'VALID'\n",
    "fc = 256\n",
    "\n",
    "# TF Placeholders & Variables\n",
    "X = tf.placeholder(tf.float32, shape=(None, 96, 96, 1), name=\"X\")\n",
    "Y = tf.placeholder(dtype=tf.float32, shape=[None, n_actions],name=\"Y\")\n",
    "eps_rewards = tf.placeholder(dtype=tf.float32, shape=[None,1], name=\"Episode_Discounted_Rewards\")\n",
    "tf_g = tf.Variable(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a very popular convnet also used for the famous DQN algorithm. Our network will input a processed resized image of 96x96 pixel, output 16 convolutions of a 8x8 kernel with a stride of 4, followed by 32 convolutions with a 4x4 kernel and a stride of 2, finished with a fully connected layer of 256 neurons.  For the convolutional layers we will use ‘VALID’ padding which will shrink the image quite aggressively. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONVOLUTION 1 - 1\n",
    "with tf.name_scope('conv1'):\n",
    "    \n",
    "    filter1 = tf.Variable(tf.truncated_normal([kerns[0], kerns[0], 1, convs[0]], dtype=tf.float32,\n",
    "                            stddev=1/np.sqrt(96**2)), name='weights1')\n",
    "    stride = [1,strides[0],strides[0],1]\n",
    "    conv = tf.nn.conv2d(X, filter1, stride, padding=pads)\n",
    "    biases = tf.Variable(tf.constant(0.0, shape=[convs[0]], dtype=tf.float32),\n",
    "                         trainable=True, name='biases1')\n",
    "    out = tf.nn.bias_add(conv, biases)\n",
    "    conv1 = tf.nn.relu(out)\n",
    "    \n",
    "    \n",
    "# CONVOLUTION 1 - 2\n",
    "with tf.name_scope('conv2'):\n",
    "    shape = int(np.prod(conv1.get_shape()[1:]))\n",
    "    filter2 = tf.Variable(tf.truncated_normal([kerns[1], kerns[1], convs[0], convs[1]], dtype=tf.float32,\n",
    "                                                stddev=1/np.sqrt(shape)), name='weights2')\n",
    "    stride = [1,strides[1],strides[1],1]\n",
    "    conv = tf.nn.conv2d(conv1, filter2, stride, padding=pads)\n",
    "    biases = tf.Variable(tf.constant(0.0, shape=[convs[1]], dtype=tf.float32),\n",
    "                         trainable=True, name='biases2')\n",
    "    out = tf.nn.bias_add(conv, biases)\n",
    "    conv2 = tf.nn.relu(out)\n",
    "    \n",
    "\n",
    "#FULLY CONNECTED 1\n",
    "with tf.name_scope('fc1') as scope:\n",
    "    shape = int(np.prod(conv2.get_shape()[1:]))\n",
    "    fc1w = tf.Variable(tf.truncated_normal([shape, fc], dtype=tf.float32, stddev=1/np.sqrt(shape)), name='weights3')\n",
    "    fc1b = tf.Variable(tf.constant(1.0, shape=[fc], dtype=tf.float32),\n",
    "                       trainable=True, name='biases3')\n",
    "    flat = tf.reshape(conv2, [-1, shape])\n",
    "    out = tf.nn.bias_add(tf.matmul(flat, fc1w), fc1b)\n",
    "    fc_1 = tf.nn.relu(out)\n",
    "    \n",
    "\n",
    "#FULLY CONNECTED 2 & SOFTMAX OUTPUT\n",
    "with tf.name_scope('softmax') as scope:\n",
    "    fc2w = tf.Variable(tf.truncated_normal([fc, n_actions], dtype=tf.float32,\n",
    "                                           stddev=1/np.sqrt(fc)), name='weights4')\n",
    "    fc2b = tf.Variable(tf.constant(1.0, shape=[n_actions], dtype=tf.float32),\n",
    "                       trainable=True, name='biases4')\n",
    "    Ylogits = tf.nn.bias_add(tf.matmul(fc_1, fc2w), fc2b)\n",
    "    output = tf.nn.softmax(Ylogits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for resizing image\n",
    "def resize(image):\n",
    "    # Greyscale Image\n",
    "    x = np.mean(image,-1)\n",
    "    # Crop Image\n",
    "    x = x[:400,100:540]\n",
    "    # Normalize Pixel Values\n",
    "    x = x/255\n",
    "    x = scipy.misc.imresize(x, [96,96])\n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply discount to episode rewards & normalize\n",
    "def discount_rewards(rewards, gamma):\n",
    "    discount = np.zeros_like(rewards)\n",
    "    G = 0\n",
    "    for i in reversed(range(0, len(rewards))):\n",
    "        G = G * gamma + rewards[i]\n",
    "        discount[i] = G\n",
    "    # Normalize \n",
    "    mean = np.mean(discount)\n",
    "    std = np.std(discount)\n",
    "    discount = (discount - mean) / (std)\n",
    "    return discount\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We measure our loss by taking the negative log of our action probability output and multiply it by a one hot vector for the action (this then only measures the loss for the given action).  As confidence in an action approaches one, the loss approaches zero.  As confidence in an action approaches zero the loss approaches infinity.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss\n",
    "loss = -tf.log(output)*Y\n",
    "loss_mean = tf.reduce_mean(loss)\n",
    "optimizer = tf.train.AdamOptimizer(alpha)\n",
    "grads = optimizer.compute_gradients(loss, var_list=tf.trainable_variables(), \n",
    "                                    grad_loss=eps_rewards)\n",
    "train = optimizer.apply_gradients(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Session and initialize variables\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup TensorBoard Writer\n",
    "writer = tf.summary.FileWriter(\"/tmp/dpg\")\n",
    "writer.add_graph(sess.graph)\n",
    "tf.summary.scalar('Loss', loss_mean)\n",
    "tf.summary.scalar('Episode_Reward', tf_g)\n",
    "tf.summary.histogram(\"Weights_1\", filter1)\n",
    "tf.summary.histogram(\"Weights_2\", filter2)\n",
    "write_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model if exists\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "load_was_success = True \n",
    "try:\n",
    "    save_dir = '/'.join(save_path.split('/')[:-1])\n",
    "    ckpt = tf.train.get_checkpoint_state(save_dir)\n",
    "    load_path = ckpt.model_checkpoint_path\n",
    "    saver.restore(sess, load_path)\n",
    "except:\n",
    "    print(\"no saved model to load. starting new session\")\n",
    "    load_was_success = False\n",
    "else:\n",
    "    print(\"loaded model: {}\".format(load_path))\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    episode_num = int(load_path.split('-')[-1])+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our three actions of moving forward, turning left & turning right\n",
    "choice = [[0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "          [0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "          [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]]\n",
    "\n",
    "# If you want your agent to learn a good policy much faster \n",
    "# combine your turning actions with moving forward\n",
    "#choice = [[0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "#          [0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "#          [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "state = resize(env.reset())\n",
    "\n",
    "for i in range(n_episodes):\n",
    "    \n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        \n",
    "        # Get action probability from network\n",
    "        feed = {X:state.reshape(1, 96, 96, 1)}\n",
    "        aprob = sess.run([output], feed_dict=feed)\n",
    "        action = np.random.choice(n_actions, p=aprob[0][0])\n",
    "        #print(aprob)\n",
    "         \n",
    "        # Convert action to one hot vector\n",
    "        oneHot = np.zeros(n_actions)\n",
    "        oneHot[action] = 1\n",
    "\n",
    "        # Perform action loop & store results\n",
    "        for i in range(actionRepeat):\n",
    "            state2, reward2, done, info = env.step(choice[action])\n",
    "            reward += reward2\n",
    "        G += reward\n",
    "        \n",
    "        # Record history\n",
    "        states.append(state)\n",
    "        actions.append(oneHot)\n",
    "        rewards.append(reward)\n",
    "        reward = 0\n",
    "        \n",
    "        # Update current state\n",
    "        state = resize(state2)\n",
    "    \n",
    "        if done:\n",
    "            average.append(G)\n",
    "            rewards = discount_rewards(rewards,gamma)\n",
    "                \n",
    "            # Define our network feed & measure average episode loss\n",
    "            feed = {X: np.dstack(states).reshape(len(states),96,96,1), \n",
    "                    eps_rewards: np.vstack(rewards), \n",
    "                    Y: np.vstack(actions)}\n",
    "            losses = sess.run([loss_mean], feed_dict=feed)\n",
    "            avg_loss.append(losses)\n",
    "            \n",
    "            if episode_num % 1 == 0:\n",
    "                print('Episode: {}   G:{:4.0f}  Average: {:4.1f}  Avg. Eps. Loss: {:4.4f}'\n",
    "                      .format(episode_num, G, np.mean(average), losses[0]))\n",
    "                \n",
    "            if episode_num % 5 == 0:\n",
    "                # Write TF Summaries\n",
    "                tf_g = tf.assign(tf_g, G)\n",
    "                sess.run(tf_g)\n",
    "                summary = sess.run(write_op, feed_dict=feed)\n",
    "                writer.add_summary(summary, episode_num)\n",
    "                writer.flush()\n",
    "            \n",
    "            if episode_num % 50 == 0:\n",
    "                # Save TensorFlow Variables \n",
    "                saver.save(sess, save_path, global_step=episode_num)\n",
    "                print(\"SAVED MODEL #{}\".format(episode_num))\n",
    "                if counter >=100:\n",
    "                    print('Last 100 Average {}'\n",
    "                          .format(np.mean(average[counter-100:counter])))\n",
    "            \n",
    "            if train:\n",
    "                # If train == True we will update the network every episode\n",
    "                _ = sess.run([train], feed_dict=feed)\n",
    "            \n",
    "            \n",
    "            # Reset our variables for next episode\n",
    "            states, actions, rewards = [], [], []\n",
    "            G = 0\n",
    "            state = resize(env.reset())\n",
    "            episode_num += 1\n",
    "            counter += 1\n",
    "            break\n",
    "    \n",
    "    if counter >=100:\n",
    "        if np.sum(average[counter-100:counter])/100>=1000:\n",
    "            print('Solved in {} Episodes'.format(episode_num))\n",
    "            saver.save(sess, save_path, global_step=episode_num)\n",
    "            print(\"SAVED MODEL #{}\".format(episode_num))\n",
    "            break\n",
    "            \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
